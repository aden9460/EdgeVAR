nohup: ignoring input
W0730 17:59:45.664007 140453739033664 torch/distributed/run.py:757] 
W0730 17:59:45.664007 140453739033664 torch/distributed/run.py:757] *****************************************
W0730 17:59:45.664007 140453739033664 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0730 17:59:45.664007 140453739033664 torch/distributed/run.py:757] *****************************************
[dist initialize] mp method=spawn
[dist initialize] mp method=spawn
[dist initialize] mp method=spawn
[dist initialize] mp method=spawn
[dist initialize] mp method=spawn
[dist initialize] mp method=spawn[dist initialize] mp method=spawn

[lrk=1, rk=1]
[lrk=3, rk=3]
[lrk=0, rk=0]
[lrk=2, rk=2]
[lrk=4, rk=4]
[lrk=6, rk=6]
[lrk=5, rk=5]
[07-31 01:59:49] (AR/VAR/utils/arg_util.py, line 179)=> [tf32] [precis] torch.get_float32_matmul_precision(): high
[07-31 01:59:49] (AR/VAR/utils/arg_util.py, line 180)=> [tf32] [ conv ] torch.backends.cudnn.allow_tf32: True
[07-31 01:59:49] (AR/VAR/utils/arg_util.py, line 181)=> [tf32] [matmul] torch.backends.cuda.matmul.allow_tf32: True
[07-31 01:59:49] (train.py                , line  41)=> global bs=140, local bs=20
[07-31 01:59:49] (train.py                , line  42)=> initial args:
{
  data_path           : /home/wangzefang/Datasets/ImageNet-1K
  exp_name            : text
  vfast               : 0
  vae_path            : /home/wangzefang/Project/distilled_decoding/VAR/model_zoo/original_VAR/model_zoo/vae_ch160v4096z32.pth
  var_path            : /home/wangzefang/edgevar/EdgeVAR/slimgpt_pub/output/sparsity_model/d24_0.2sparsity_150i_256eva_scale_slimgptmethod_nocompensate_temporary.pth
  tfast               : 0
  depth               : 24
  sparsity            : 0.2
  ini                 : -1
  hd                  : 0.02
  aln                 : 0.5
  alng                : 0.001
  fp16                : 1
  tblr                : 0.0001
  tlr                 : 5.46875e-05
  twd                 : 0.05
  twde                : 0.05
  tclip               : 2.0
  ls                  : 0.0
  maxlayer            : 16
  bs                  : 140
  batch_size          : 20
  glb_batch_size      : 140
  ac                  : 1
  ep                  : 1
  wp                  : 0.02
  wp0                 : 0.005
  wpe                 : 0.1
  sche                : lin0
  opt                 : adamw
  afuse               : True
  saln                : False
  anorm               : True
  fuse                : True
  pn                  : 1_2_3_4_5_6_8_10_13_16
  patch_size          : 16
  patch_nums          : (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)
  resos               : (16, 32, 48, 64, 80, 96, 128, 160, 208, 256)
  data_load_reso      : 256
  mid_reso            : 1.125
  hflip               : False
  workers             : 0
  pg                  : 0.0
  pg0                 : 4
  pgwp                : 0.0033333333333333335
  cmd                 : --depth=24 --bs=140 --ep=1 --fp16=1 --alng=1e-3 --wpe=0.1 --sparsity=0.2 --local_out_dir_path=/home/wangzefang/edgevar/EdgeVAR/VAR/traind_model/d24_0.2_nocompenstate_1epoch_temporary --data_path=/home/wangzefang/Datasets/ImageNet-1K --var_path=/home/wangzefang/edgevar/EdgeVAR/slimgpt_pub/output/sparsity_model/d24_0.2sparsity_150i_256eva_scale_slimgptmethod_nocompensate_temporary.pth --vae_path=/home/wangzefang/Project/distilled_decoding/VAR/model_zoo/original_VAR/model_zoo/vae_ch160v4096z32.pth
  branch              : main
  commit_id           : 5310337f2a002d3696f7eee8724dfeb127624060
  commit_msg          : sixth
  acc_mean            : None
  acc_tail            : None
  L_mean              : None
  L_tail              : None
  vacc_mean           : None
  vacc_tail           : None
  vL_mean             : None
  vL_tail             : None
  grad_norm           : None
  cur_lr              : None
  cur_wd              : None
  cur_it              : 
  cur_ep              : 
  remain_time         : 
  finish_time         : 
  local_out_dir_path  : /home/wangzefang/edgevar/EdgeVAR/VAR/traind_model/d24_0.2_nocompenstate_1epoch_temporary
  tb_log_dir_path     : /home/wangzefang/edgevar/EdgeVAR/VAR/traind_model/d24_0.2_nocompenstate_1epoch_temporary/tb-VARd24__pn1_2_3_4_5_6_8_10_13_16__b140ep1adamlr0.0001wd0.05
  log_txt_path        : /home/wangzefang/edgevar/EdgeVAR/VAR/traind_model/d24_0.2_nocompenstate_1epoch_temporary/log.txt
  last_ckpt_path      : /home/wangzefang/edgevar/EdgeVAR/VAR/traind_model/d24_0.2_nocompenstate_1epoch_temporary/ar-ckpt-last.pth
  tf32                : True
  seed                : None
  transfer            : False
  same_seed_for_all_ranks: 0
  local_debug         : False
  dbg_nan             : False
}

[07-31 01:59:49] (train.py                , line  46)=> [build PT data] ...

[07-31 01:59:52] (dgeVAR/VAR/utils/data.py, line  34)=> [Dataset] len(train_set)=1281167, len(val_set)=50000, num_classes=1000
[07-31 01:59:52] (dgeVAR/VAR/utils/data.py, line  48)=> Transform [train] = 
[07-31 01:59:52] (dgeVAR/VAR/utils/data.py, line  51)=> Resize(size=288, interpolation=lanczos, max_size=None, antialias=True)
[07-31 01:59:52] (dgeVAR/VAR/utils/data.py, line  51)=> RandomCrop(size=(256, 256), padding=None)
[07-31 01:59:52] (dgeVAR/VAR/utils/data.py, line  51)=> ToTensor()
[07-31 01:59:52] (dgeVAR/VAR/utils/data.py, line  51)=> <function normalize_01_into_pm1 at 0x7f199c1470d0>
[07-31 01:59:52] (dgeVAR/VAR/utils/data.py, line  54)=> ---------------------------

[07-31 01:59:52] (dgeVAR/VAR/utils/data.py, line  48)=> Transform [val] = 
[07-31 01:59:52] (dgeVAR/VAR/utils/data.py, line  51)=> Resize(size=288, interpolation=lanczos, max_size=None, antialias=True)
[07-31 01:59:52] (dgeVAR/VAR/utils/data.py, line  51)=> CenterCrop(size=(256, 256))
[07-31 01:59:52] (dgeVAR/VAR/utils/data.py, line  51)=> ToTensor()
[07-31 01:59:52] (dgeVAR/VAR/utils/data.py, line  51)=> <function normalize_01_into_pm1 at 0x7f199c1470d0>
[07-31 01:59:52] (dgeVAR/VAR/utils/data.py, line  54)=> ---------------------------

[07-31 01:59:52] (train.py                , line  69)=> [auto_resume] no ckpt found @ /home/wangzefang/edgevar/EdgeVAR/VAR/traind_model/d24_0.2_nocompenstate_1epoch_temporary/ar-ckpt*.pth
[07-31 01:59:52] (train.py                , line  69)=> [auto_resume quit]
[07-31 01:59:52] (train.py                , line  70)=> [dataloader multi processing] ...     [dataloader multi processing](*) finished! (0.00s)
[07-31 01:59:52] (train.py                , line  76)=> [dataloader] gbs=140, lbs=20, iters_train=9152, types(tr, va)=('DatasetFolder', 'DatasetFolder')
[07-31 01:59:52] (dgeVAR/VAR/models/var.py, line 102)=> 
[constructor]  ==== flash_if_available=True (0/24), fused_if_available=True (fusing_add_ln=0/24, fusing_mlp=0/24) ==== 
    [VAR config ] embed_dim=1536, num_heads=24, depth=24, mlp_ratio=4.0
    [drop ratios ] drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.1 (tensor([0.0000, 0.0043, 0.0087, 0.0130, 0.0174, 0.0217, 0.0261, 0.0304, 0.0348,
        0.0391, 0.0435, 0.0478, 0.0522, 0.0565, 0.0609, 0.0652, 0.0696, 0.0739,
        0.0783, 0.0826, 0.0870, 0.0913, 0.0957, 0.1000]))

[07-31 01:59:53] (dgeVAR/VAR/models/var.py, line 259)=> [init_weights] VAR with init_std=0.0147314
[07-31 01:59:55] (train.py                , line 108)=> 加载原始模型权重...
[07-31 01:59:56] (train.py                , line 124)=> [INIT] VAR model = VAR(
  drop_path_rate=0.1
  (word_embed): Linear(in_features=32, out_features=1536, bias=True)
  (class_emb): Embedding(1001, 1536)
  (lvl_embed): Embedding(10, 1536)
  (shared_ada_lin): Identity()
  (blocks): ModuleList(
    (0): AdaLNSelfAttn(
      shared_aln=False
      (drop_path): Identity()
      (attn): SelfAttention(
        using_flash=False, using_xform=False, attn_l2_norm=True
        (mat_qkv): Linear(in_features=1536, out_features=4608, bias=False)
        (proj): Linear(in_features=1536, out_features=1536, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp_func=False
        (fc1): Linear(in_features=1536, out_features=4915, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=4915, out_features=1536, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
      (ada_lin): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1536, out_features=9216, bias=True)
      )
    )
    (1-23): 23 x AdaLNSelfAttn(
      shared_aln=False
      (drop_path): DropPath((drop_prob=...))
      (attn): SelfAttention(
        using_flash=False, using_xform=False, attn_l2_norm=True
        (mat_qkv): Linear(in_features=1536, out_features=4608, bias=False)
        (proj): Linear(in_features=1536, out_features=1536, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp_func=False
        (fc1): Linear(in_features=1536, out_features=4915, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=4915, out_features=1536, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
      (ada_lin): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1536, out_features=9216, bias=True)
      )
    )
  )
  (head_nm): AdaLNBeforeHead(
    (ln_wo_grad): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
    (ada_lin): Sequential(
      (0): SiLU()
      (1): Linear(in_features=1536, out_features=3072, bias=True)
    )
  )
  (head): Linear(in_features=1536, out_features=4096, bias=True)
)


[07-31 01:59:56] (train.py                , line 126)=> [INIT][#para] VAE=108.95, VAE.enc=44.11, VAE.dec=64.65, VAE.quant=0.17
[07-31 01:59:56] (train.py                , line 127)=> [INIT][#para] VAR=942.76


[07-31 01:59:56] (/VAR/utils/lr_control.py, line  99)=> [get_param_groups] param_groups = 
{ 'D': { 'lr_sc': 1.0,
         'params': "('word_embed.weight, class_emb.weight, blocks.0.attn.mat_qkv.weight, blocks.0.attn.proj.weight, blocks.0.ffn.fc1.weight, blocks.0.ffn.fc2.weight, blocks.0.ada_lin.1.weight, '\n"
                   " 'blocks.1.attn.mat_qkv.weight, blocks.1.attn.proj.weight, blocks.1.ffn.fc1.weight, blocks.1.ffn.fc2.weight, blocks.1.ada_lin.1.weight, blocks.2.attn.mat_qkv.weight, blocks.2.attn.proj.weight, '\n"
                   " 'blocks.2.ffn.fc1.weight, blocks.2.ffn.fc2.weight, blocks.2.ada_lin.1.weight, blocks.3.attn.mat_qkv.weight, blocks.3.attn.proj.weight, blocks.3.ffn.fc1.weight, blocks.3.ffn.fc2.weight, '\n"
                   " 'blocks.3.ada_lin.1.weight, blocks.4.attn.mat_qkv.weight, blocks.4.attn.proj.weight, blocks.4.ffn.fc1.weight, blocks.4.ffn.fc2.weight, blocks.4.ada_lin.1.weight, blocks.5.attn.mat_qkv.weight, '\n"
                   " 'blocks.5.attn.proj.weight, blocks.5.ffn.fc1.weight, blocks.5.ffn.fc2.weight, blocks.5.ada_lin.1.weight, blocks.6.attn.mat_qkv.weight, blocks.6.attn.proj.weight, blocks.6.ffn.fc1.weight, '\n"
                   " 'blocks.6.ffn.fc2.weight, blocks.6.ada_lin.1.weight, blocks.7.attn.mat_qkv.weight, blocks.7.attn.proj.weight, blocks.7.ffn.fc1.weight, blocks.7.ffn.fc2.weight, blocks.7.ada_lin.1.weight, '\n"
                   " 'blocks.8.attn.mat_qkv.weight, blocks.8.attn.proj.weight, blocks.8.ffn.fc1.weight, blocks.8.ffn.fc2.weight, blocks.8.ada_lin.1.weight, blocks.9.attn.mat_qkv.weight, blocks.9.attn.proj.weight, '\n"
                   " 'blocks.9.ffn.fc1.weight, blocks.9.ffn.fc2.weight, blocks.9.ada_lin.1.weight, blocks.10.attn.mat_qkv.weight, blocks.10.attn.proj.weight, blocks.10.ffn.fc1.weight, blocks.10.ffn.fc2.weight, '\n"
                   " 'blocks.10.ada_lin.1.weight, blocks.11.attn.mat_qkv.weight, blocks.11.attn.proj.weight, blocks.11.ffn.fc1.weight, blocks.11.ffn.fc2.weight, blocks.11.ada_lin.1.weight, '\n"
                   " 'blocks.12.attn.mat_qkv.weight, blocks.12.attn.proj.weight, blocks.12.ffn.fc1.weight, blocks.12.ffn.fc2.weight, blocks.12.ada_lin.1.weight, blocks.13.attn.mat_qkv.weight, '\n"
                   " 'blocks.13.attn.proj.weight, blocks.13.ffn.fc1.weight, blocks.13.ffn.fc2.weight, blocks.13.ada_lin.1.weight, blocks.14.attn.mat_qkv.weight, blocks.14.attn.proj.weight, blocks.14.ffn.fc1.weight, '\n"
                   " 'blocks.14.ffn.fc2.weight, blocks.14.ada_lin.1.weight, blocks.15.attn.mat_qkv.weight, blocks.15.attn.proj.weight, blocks.15.ffn.fc1.weight, blocks.15.ffn.fc2.weight, blocks.15.ada_lin.1.weight, '\n"
                   " 'blocks.16.attn.mat_qkv.weight, blocks.16.attn.proj.weight, blocks.16.ffn.fc1.weight, blocks.16.ffn.fc2.weight, blocks.16.ada_lin.1.weight, blocks.17.attn.mat_qkv.weight, '\n"
                   " 'blocks.17.attn.proj.weight, blocks.17.ffn.fc1.weight, blocks.17.ffn.fc2.weight, blocks.17.ada_lin.1.weight, blocks.18.attn.mat_qkv.weight, blocks.18.attn.proj.weight, blocks.18.ffn.fc1.weight, '\n"
                   " 'blocks.18.ffn.fc2.weight, blocks.18.ada_lin.1.weight, blocks.19.attn.mat_qkv.weight, blocks.19.attn.proj.weight, blocks.19.ffn.fc1.weight, blocks.19.ffn.fc2.weight, blocks.19.ada_lin.1.weight, '\n"
                   " 'blocks.20.attn.mat_qkv.weight, blocks.20.attn.proj.weight, blocks.20.ffn.fc1.weight, blocks.20.ffn.fc2.weight, blocks.20.ada_lin.1.weight, blocks.21.attn.mat_qkv.weight, '\n"
                   " 'blocks.21.attn.proj.weight, blocks.21.ffn.fc1.weight, blocks.21.ffn.fc2.weight, blocks.21.ada_lin.1.weight, blocks.22.attn.mat_qkv.weight, blocks.22.attn.proj.weight, blocks.22.ffn.fc1.weight, '\n"
                   " 'blocks.22.ffn.fc2.weight, blocks.22.ada_lin.1.weight, blocks.23.attn.mat_qkv.weight, blocks.23.attn.proj.weight, blocks.23.ffn.fc1.weight, blocks.23.ffn.fc2.weight, blocks.23.ada_lin.1.weight, '\n"
                   " 'head_nm.ada_lin.1.weight, head.weight')",
         'wd_sc': 1.0},
  'ND': { 'lr_sc': 1.0,
          'params': "('pos_start, pos_1LC, word_embed.bias, lvl_embed.weight, blocks.0.attn.scale_mul_1H11, blocks.0.attn.q_bias, blocks.0.attn.v_bias, blocks.0.attn.proj.bias, blocks.0.ffn.fc1.bias, '\n"
                    " 'blocks.0.ffn.fc2.bias, blocks.0.ada_lin.1.bias, blocks.1.attn.scale_mul_1H11, blocks.1.attn.q_bias, blocks.1.attn.v_bias, blocks.1.attn.proj.bias, blocks.1.ffn.fc1.bias, blocks.1.ffn.fc2.bias, '\n"
                    " 'blocks.1.ada_lin.1.bias, blocks.2.attn.scale_mul_1H11, blocks.2.attn.q_bias, blocks.2.attn.v_bias, blocks.2.attn.proj.bias, blocks.2.ffn.fc1.bias, blocks.2.ffn.fc2.bias, blocks.2.ada_lin.1.bias, '\n"
                    " 'blocks.3.attn.scale_mul_1H11, blocks.3.attn.q_bias, blocks.3.attn.v_bias, blocks.3.attn.proj.bias, blocks.3.ffn.fc1.bias, blocks.3.ffn.fc2.bias, blocks.3.ada_lin.1.bias, '\n"
                    " 'blocks.4.attn.scale_mul_1H11, blocks.4.attn.q_bias, blocks.4.attn.v_bias, blocks.4.attn.proj.bias, blocks.4.ffn.fc1.bias, blocks.4.ffn.fc2.bias, blocks.4.ada_lin.1.bias, '\n"
                    " 'blocks.5.attn.scale_mul_1H11, blocks.5.attn.q_bias, blocks.5.attn.v_bias, blocks.5.attn.proj.bias, blocks.5.ffn.fc1.bias, blocks.5.ffn.fc2.bias, blocks.5.ada_lin.1.bias, '\n"
                    " 'blocks.6.attn.scale_mul_1H11, blocks.6.attn.q_bias, blocks.6.attn.v_bias, blocks.6.attn.proj.bias, blocks.6.ffn.fc1.bias, blocks.6.ffn.fc2.bias, blocks.6.ada_lin.1.bias, '\n"
                    " 'blocks.7.attn.scale_mul_1H11, blocks.7.attn.q_bias, blocks.7.attn.v_bias, blocks.7.attn.proj.bias, blocks.7.ffn.fc1.bias, blocks.7.ffn.fc2.bias, blocks.7.ada_lin.1.bias, '\n"
                    " 'blocks.8.attn.scale_mul_1H11, blocks.8.attn.q_bias, blocks.8.attn.v_bias, blocks.8.attn.proj.bias, blocks.8.ffn.fc1.bias, blocks.8.ffn.fc2.bias, blocks.8.ada_lin.1.bias, '\n"
                    " 'blocks.9.attn.scale_mul_1H11, blocks.9.attn.q_bias, blocks.9.attn.v_bias, blocks.9.attn.proj.bias, blocks.9.ffn.fc1.bias, blocks.9.ffn.fc2.bias, blocks.9.ada_lin.1.bias, '\n"
                    " 'blocks.10.attn.scale_mul_1H11, blocks.10.attn.q_bias, blocks.10.attn.v_bias, blocks.10.attn.proj.bias, blocks.10.ffn.fc1.bias, blocks.10.ffn.fc2.bias, blocks.10.ada_lin.1.bias, '\n"
                    " 'blocks.11.attn.scale_mul_1H11, blocks.11.attn.q_bias, blocks.11.attn.v_bias, blocks.11.attn.proj.bias, blocks.11.ffn.fc1.bias, blocks.11.ffn.fc2.bias, blocks.11.ada_lin.1.bias, '\n"
                    " 'blocks.12.attn.scale_mul_1H11, blocks.12.attn.q_bias, blocks.12.attn.v_bias, blocks.12.attn.proj.bias, blocks.12.ffn.fc1.bias, blocks.12.ffn.fc2.bias, blocks.12.ada_lin.1.bias, '\n"
                    " 'blocks.13.attn.scale_mul_1H11, blocks.13.attn.q_bias, blocks.13.attn.v_bias, blocks.13.attn.proj.bias, blocks.13.ffn.fc1.bias, blocks.13.ffn.fc2.bias, blocks.13.ada_lin.1.bias, '\n"
                    " 'blocks.14.attn.scale_mul_1H11, blocks.14.attn.q_bias, blocks.14.attn.v_bias, blocks.14.attn.proj.bias, blocks.14.ffn.fc1.bias, blocks.14.ffn.fc2.bias, blocks.14.ada_lin.1.bias, '\n"
                    " 'blocks.15.attn.scale_mul_1H11, blocks.15.attn.q_bias, blocks.15.attn.v_bias, blocks.15.attn.proj.bias, blocks.15.ffn.fc1.bias, blocks.15.ffn.fc2.bias, blocks.15.ada_lin.1.bias, '\n"
                    " 'blocks.16.attn.scale_mul_1H11, blocks.16.attn.q_bias, blocks.16.attn.v_bias, blocks.16.attn.proj.bias, blocks.16.ffn.fc1.bias, blocks.16.ffn.fc2.bias, blocks.16.ada_lin.1.bias, '\n"
                    " 'blocks.17.attn.scale_mul_1H11, blocks.17.attn.q_bias, blocks.17.attn.v_bias, blocks.17.attn.proj.bias, blocks.17.ffn.fc1.bias, blocks.17.ffn.fc2.bias, blocks.17.ada_lin.1.bias, '\n"
                    " 'blocks.18.attn.scale_mul_1H11, blocks.18.attn.q_bias, blocks.18.attn.v_bias, blocks.18.attn.proj.bias, blocks.18.ffn.fc1.bias, blocks.18.ffn.fc2.bias, blocks.18.ada_lin.1.bias, '\n"
                    " 'blocks.19.attn.scale_mul_1H11, blocks.19.attn.q_bias, blocks.19.attn.v_bias, blocks.19.attn.proj.bias, blocks.19.ffn.fc1.bias, blocks.19.ffn.fc2.bias, blocks.19.ada_lin.1.bias, '\n"
                    " 'blocks.20.attn.scale_mul_1H11, blocks.20.attn.q_bias, blocks.20.attn.v_bias, blocks.20.attn.proj.bias, blocks.20.ffn.fc1.bias, blocks.20.ffn.fc2.bias, blocks.20.ada_lin.1.bias, '\n"
                    " 'blocks.21.attn.scale_mul_1H11, blocks.21.attn.q_bias, blocks.21.attn.v_bias, blocks.21.attn.proj.bias, blocks.21.ffn.fc1.bias, blocks.21.ffn.fc2.bias, blocks.21.ada_lin.1.bias, '\n"
                    " 'blocks.22.attn.scale_mul_1H11, blocks.22.attn.q_bias, blocks.22.attn.v_bias, blocks.22.attn.proj.bias, blocks.22.ffn.fc1.bias, blocks.22.ffn.fc2.bias, blocks.22.ada_lin.1.bias, '\n"
                    " 'blocks.23.attn.scale_mul_1H11, blocks.23.attn.q_bias, blocks.23.attn.v_bias, blocks.23.attn.proj.bias, blocks.23.ffn.fc1.bias, blocks.23.ffn.fc2.bias, blocks.23.ada_lin.1.bias, '\n"
                    " 'head_nm.ada_lin.1.bias, head.bias')",
          'wd_sc': 0.0}}

[07-31 02:00:00] (/VAR/utils/lr_control.py, line 104)=> [get_param_groups][rank0] type(model).__name__='VAR' count=298, numel=942758152
[07-31 02:00:00] (/VAR/utils/lr_control.py, line 104)=> [get_param_groups][rank1] type(model).__name__='VAR' count=298, numel=942758152
[07-31 02:00:00] (/VAR/utils/lr_control.py, line 104)=> [get_param_groups][rank2] type(model).__name__='VAR' count=298, numel=942758152
[07-31 02:00:00] (/VAR/utils/lr_control.py, line 104)=> [get_param_groups][rank3] type(model).__name__='VAR' count=298, numel=942758152
[07-31 02:00:00] (/VAR/utils/lr_control.py, line 104)=> [get_param_groups][rank4] type(model).__name__='VAR' count=298, numel=942758152
[07-31 02:00:00] (/VAR/utils/lr_control.py, line 104)=> [get_param_groups][rank5] type(model).__name__='VAR' count=298, numel=942758152
[07-31 02:00:00] (/VAR/utils/lr_control.py, line 105)=> 
[07-31 02:00:00] (train.py                , line 142)=> [INIT] optim=functools.partial(<class 'torch.optim.adamw.AdamW'>, betas=(0.9, 0.95), fused=True), opt_kw={'lr': 5.46875e-05, 'weight_decay': 0}

[07-31 02:00:00] (/VAR/utils/lr_control.py, line 104)=> [get_param_groups][rank6] type(model).__name__='VAR' count=298, numel=942758152
[07-31 02:00:11] (dgeVAR/VAR/utils/misc.py, line 314)=> [Ep]: [   0/1]  [   0/9152]  eta: 1 day, 1:41:34  tlr: 2.7e-07  tnm: 8.96  Lm: 8.564 (8.564)  Lt: 8.269 (8.269)  Accm: 0.12 (0.12)  Acct: 0.08 (0.08)  time: 10.1064  data: 0.2996
W0730 18:16:16.799381 140453739033664 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 2249149 closing signal SIGTERM
W0730 18:16:16.800021 140453739033664 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 2249150 closing signal SIGTERM
W0730 18:16:16.800722 140453739033664 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 2249151 closing signal SIGTERM
W0730 18:16:16.801478 140453739033664 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 2249154 closing signal SIGTERM
W0730 18:16:16.802696 140453739033664 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 2249156 closing signal SIGTERM
W0730 18:16:16.802852 140453739033664 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 2249158 closing signal SIGTERM
E0730 18:16:18.821221 140453739033664 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: -9) local_rank: 3 (pid: 2249152) of binary: /home/wangzefang/miniconda3/envs/distill/bin/python
Traceback (most recent call last):
  File "/home/wangzefang/miniconda3/envs/distill/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.3.0', 'console_scripts', 'torchrun')())
  File "/home/wangzefang/miniconda3/envs/distill/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/home/wangzefang/miniconda3/envs/distill/lib/python3.8/site-packages/torch/distributed/run.py", line 879, in main
    run(args)
  File "/home/wangzefang/miniconda3/envs/distill/lib/python3.8/site-packages/torch/distributed/run.py", line 870, in run
    elastic_launch(
  File "/home/wangzefang/miniconda3/envs/distill/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/wangzefang/miniconda3/envs/distill/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 263, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
train.py FAILED
--------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-07-30_18:16:16
  host      : ubuntu
  rank      : 3 (local_rank: 3)
  exitcode  : -9 (pid: 2249152)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 2249152
========================================================
